# Project Snapshot: Bussikartta
# Git Commit: 507bb7eada327c77629d424341af83fb32be57ae
# Branch: main
# Generated: 2025-06-18T23:52:29Z

# Files in this snapshot:
25.1.1
api/db.py
api/main.py
api/requirements.txt
api/routes/agency.py
api/routes/alerts.py
api/routes/calendar.py
api/routes/emissions.py
api/routes/fare_attributes.py
api/routes/fare_rules.py
api/routes/feed_info.py
api/routes/__init__.py
api/routes/routes.py
api/routes/stops.py
api/routes/transfers.py
api/routes/trips.py
api/routes/vehicle_positions.py
api/routes/vehicles.py
backup.sh
create_snapshot.sh
docker-compose.yaml
docker-compose.yaml.bak
Dockerfile
docs/AI-Guidelines.md
docs/developer_handbook.md
docs/frontend_architecture.md
docs/gtfs_data_handling.md
docs/overview.md
docs/project_architecture.md
docs/project_architecture_plan.md
docs/services.md
feed.pb
.gitignore
gtfs_static/Dockerfile
gtfs_static/main.py
gtfs_static/requirements.txt
ingestion/config.py
ingestion/gtfs_realtime_pb2.py
ingestion/__init__.py
ingestion/mqtt_hfp_ingest/Dockerfile
ingestion/mqtt_hfp_ingest/main.py
ingestion/mqtt_hfp_ingest/requirements.txt
ingestion/vehicle_positions_ingest.py
init_timescale.sql
main.py
README.md
tools/mqtt_watchdog.sh
tools/vehicle_watchdog.sh

=== FILE: 25.1.1 ===


=== FILE: api/db.py ===
import os
import psycopg2
from psycopg2.extras import RealDictCursor

def get_db_connection():
    conn = psycopg2.connect(
        host=os.getenv("PGHOST", "repo-db-1"),
        port=os.getenv("PGPORT", "5432"),
        dbname=os.getenv("PGDATABASE", "hslbussit"),
        user=os.getenv("PGUSER", "postgres"),
        password=os.getenv("PGPASSWORD", "your_db_password")
    )
    return conn

=== FILE: api/main.py ===
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware  # <--- ADD THIS

# Import routers from all route modules
from api.routes import (
    agency,
    alerts,
    calendar,
    emissions,
    fare_attributes,
    fare_rules,
    feed_info,
    routes,
    stops,
    transfers,
    trips,
    vehicle_positions,
    vehicles,
)

app = FastAPI(
    title="HSL Bus API",
    description="API for Helsinki Regional Transport data",
    version="0.1.0",
)

# --- ADD THIS BLOCK ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://192.168.3.114:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
# ----------------------

# Include all routers
app.include_router(agency.router)
app.include_router(alerts.router)
app.include_router(calendar.router)
app.include_router(emissions.router)
app.include_router(fare_attributes.router)
app.include_router(fare_rules.router)
app.include_router(feed_info.router)
app.include_router(routes.router)
app.include_router(stops.router)
app.include_router(transfers.router)
app.include_router(trips.router)
app.include_router(vehicle_positions.router)
app.include_router(vehicles.router)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8007)

=== FILE: api/requirements.txt ===
fastapi
uvicorn
psycopg2-binary
requests
protobuf
gtfs-realtime-bindings


=== FILE: api/routes/agency.py ===
from fastapi import APIRouter

router = APIRouter()

@router.get("/agency")
def get_agency():
    return {
        "agency_id": "HSL",
        "agency_name": "Helsinki Regional Transport Authority",
        "agency_url": "https://www.hsl.fi/",
        "agency_timezone": "Europe/Helsinki",
        "agency_lang": "fi",
        "agency_phone": "+358 9 4766 4000"
    }

=== FILE: api/routes/alerts.py ===
# api/routes/alerts.py
from fastapi import APIRouter, HTTPException
import psycopg2
import os

router = APIRouter()

def get_db_connection():
    return psycopg2.connect(
        host=os.getenv("PGHOST", "db"),
        port=os.getenv("PGPORT", "5432"),
        dbname=os.getenv("PGDATABASE", "hslbussit"),
        user=os.getenv("PGUSER", "postgres"),
        password=os.getenv("PGPASSWORD", "supersecurepassword"),
    )

@router.get("/alerts")
def get_alerts():
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        cur.execute("SELECT alert_id, header_text, description_text, active_start, active_end FROM alerts WHERE active_end > NOW();")
        rows = cur.fetchall()
        alerts = []
        for row in rows:
            alerts.append({
                "alert_id": row[0],
                "header_text": row[1],
                "description_text": row[2],
                "active_start": row[3].isoformat() if row[3] else None,
                "active_end": row[4].isoformat() if row[4] else None
            })
        return alerts
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        cur.close()
        conn.close()

=== FILE: api/routes/calendar.py ===
# api/routes/calendar.py
from fastapi import APIRouter, HTTPException
import psycopg2
import os

router = APIRouter()

def get_db_connection():
    return psycopg2.connect(
        host=os.getenv("PGHOST", "db"),
        port=os.getenv("PGPORT", "5432"),
        dbname=os.getenv("PGDATABASE", "hslbussit"),
        user=os.getenv("PGUSER", "postgres"),
        password=os.getenv("PGPASSWORD", "supersecurepassword"),
    )

@router.get("/calendar")
def get_calendar():
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        cur.execute("""
            SELECT service_id, monday, tuesday, wednesday, thursday, friday, saturday, sunday, start_date, end_date
            FROM calendar;
        """)
        rows = cur.fetchall()
        calendar = []
        for row in rows:
            calendar.append({
                "service_id": row[0],
                "monday": row[1],
                "tuesday": row[2],
                "wednesday": row[3],
                "thursday": row[4],
                "friday": row[5],
                "saturday": row[6],
                "sunday": row[7],
                "start_date": row[8].isoformat() if row[8] else None,
                "end_date": row[9].isoformat() if row[9] else None,
            })
        return calendar
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        cur.close()
        conn.close()

=== FILE: api/routes/emissions.py ===
# api/routes/emissions.py
from fastapi import APIRouter, HTTPException
import psycopg2
import os

router = APIRouter()

def get_db_connection():
    return psycopg2.connect(
        host=os.getenv("PGHOST", "db"),
        port=os.getenv("PGPORT", "5432"),
        dbname=os.getenv("PGDATABASE", "hslbussit"),
        user=os.getenv("PGUSER", "postgres"),
        password=os.getenv("PGPASSWORD", "supersecurepassword"),
    )

@router.get("/emissions")
def get_emissions():
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        cur.execute("SELECT vehicle_id, emission_type, emission_value FROM emissions;")
        rows = cur.fetchall()
        emissions = []
        for row in rows:
            emissions.append({
                "vehicle_id": row[0],
                "emission_type": row[1],
                "emission_value": row[2]
            })
        return emissions
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        cur.close()
        conn.close()

=== FILE: api/routes/fare_attributes.py ===
# api/routes/fare_attributes.py
from fastapi import APIRouter, HTTPException
import psycopg2
import os

router = APIRouter()

def get_db_connection():
    return psycopg2.connect(
        host=os.getenv("PGHOST", "db"),
        port=os.getenv("PGPORT", "5432"),
        dbname=os.getenv("PGDATABASE", "hslbussit"),
        user=os.getenv("PGUSER", "postgres"),
        password=os.getenv("PGPASSWORD", "supersecurepassword"),
    )

@router.get("/fare_attributes")
def get_fare_attributes():
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        cur.execute("SELECT fare_id, price, currency_type, payment_method FROM fare_attributes;")
        rows = cur.fetchall()
        fares = []
        for row in rows:
            fares.append({
                "fare_id": row[0],
                "price": float(row[1]),
                "currency_type": row[2],
                "payment_method": row[3]
            })
        return fares
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        cur.close()
        conn.close()

=== FILE: api/routes/fare_rules.py ===
# api/routes/fare_rules.py
from fastapi import APIRouter, HTTPException
import psycopg2
import os

router = APIRouter()

def get_db_connection():
    return psycopg2.connect(
        host=os.getenv("PGHOST", "db"),
        port=os.getenv("PGPORT", "5432"),
        dbname=os.getenv("PGDATABASE", "hslbussit"),
        user=os.getenv("PGUSER", "postgres"),
        password=os.getenv("PGPASSWORD", "supersecurepassword"),
    )

@router.get("/fare_rules")
def get_fare_rules():
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        cur.execute("SELECT fare_id, origin_id, destination_id, contains_id FROM fare_rules;")
        rows = cur.fetchall()
        rules = []
        for row in rows:
            rules.append({
                "fare_id": row[0],
                "origin_id": row[1],
                "destination_id": row[2],
                "contains_id": row[3]
            })
        return rules
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        cur.close()
        conn.close()

=== FILE: api/routes/feed_info.py ===
# api/routes/feed_info.py
from fastapi import APIRouter, HTTPException
import psycopg2
import os

router = APIRouter()

def get_db_connection():
    return psycopg2.connect(
        host=os.getenv("PGHOST", "db"),
        port=os.getenv("PGPORT", "5432"),
        dbname=os.getenv("PGDATABASE", "hslbussit"),
        user=os.getenv("PGUSER", "postgres"),
        password=os.getenv("PGPASSWORD", "supersecurepassword"),
    )

@router.get("/feed_info")
def get_feed_info():
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        cur.execute("SELECT feed_publisher_name, feed_publisher_url, feed_lang, feed_version FROM feed_info;")
        row = cur.fetchone()
        if not row:
            return {}
        return {
            "feed_publisher_name": row[0],
            "feed_publisher_url": row[1],
            "feed_lang": row[2],
            "feed_version": row[3]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        cur.close()
        conn.close()

=== FILE: api/routes/__init__.py ===
from fastapi import APIRouter

router = APIRouter()

@router.get("/")
def root():
    return {"message": "__init__.py endpoint working"}


=== FILE: api/routes/routes.py ===
# api/routes/routes.py
from fastapi import APIRouter, HTTPException
import psycopg2
import os

router = APIRouter()

def get_db_connection():
    return psycopg2.connect(
        host=os.getenv("PGHOST", "db"),
        port=os.getenv("PGPORT", "5432"),
        dbname=os.getenv("PGDATABASE", "hslbussit"),
        user=os.getenv("PGUSER", "postgres"),
        password=os.getenv("PGPASSWORD", "supersecurepassword"),
    )

@router.get("/routes")
def get_routes():
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        cur.execute("SELECT route_id, route_short_name, route_long_name, route_type FROM routes;")
        rows = cur.fetchall()
        routes = []
        for row in rows:
            routes.append({
                "route_id": row[0],
                "route_short_name": row[1],
                "route_long_name": row[2],
                "route_type": row[3]
            })
        return routes
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        cur.close()
        conn.close()

=== FILE: api/routes/stops.py ===
# api/routes/stops.py
from fastapi import APIRouter, HTTPException
import psycopg2
import os

router = APIRouter()

def get_db_connection():
    return psycopg2.connect(
        host=os.getenv("PGHOST", "db"),
        port=os.getenv("PGPORT", "5432"),
        dbname=os.getenv("PGDATABASE", "hslbussit"),
        user=os.getenv("PGUSER", "postgres"),
        password=os.getenv("PGPASSWORD", "supersecurepassword"),
    )

@router.get("/stops")
def get_stops():
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        cur.execute("SELECT stop_id, stop_name, stop_lat, stop_lon FROM stops;")
        rows = cur.fetchall()
        stops = []
        for row in rows:
            stops.append({
                "stop_id": row[0],
                "stop_name": row[1],
                "stop_lat": row[2],
                "stop_lon": row[3]
            })
        return stops
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        cur.close()
        conn.close()

=== FILE: api/routes/transfers.py ===
# api/routes/transfers.py
from fastapi import APIRouter, HTTPException
import psycopg2
import os

router = APIRouter()

def get_db_connection():
    return psycopg2.connect(
        host=os.getenv("PGHOST", "db"),
        port=os.getenv("PGPORT", "5432"),
        dbname=os.getenv("PGDATABASE", "hslbussit"),
        user=os.getenv("PGUSER", "postgres"),
        password=os.getenv("PGPASSWORD", "supersecurepassword"),
    )

@router.get("/transfers")
def get_transfers():
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        cur.execute("SELECT from_stop_id, to_stop_id, transfer_type, min_transfer_time FROM transfers;")
        rows = cur.fetchall()
        transfers = []
        for row in rows:
            transfers.append({
                "from_stop_id": row[0],
                "to_stop_id": row[1],
                "transfer_type": row[2],
                "min_transfer_time": row[3]
            })
        return transfers
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        cur.close()
        conn.close()

=== FILE: api/routes/trips.py ===
# api/routes/trips.py
from fastapi import APIRouter, HTTPException
import psycopg2
import os

router = APIRouter()

def get_db_connection():
    return psycopg2.connect(
        host=os.getenv("PGHOST", "db"),
        port=os.getenv("PGPORT", "5432"),
        dbname=os.getenv("PGDATABASE", "hslbussit"),
        user=os.getenv("PGUSER", "postgres"),
        password=os.getenv("PGPASSWORD", "supersecurepassword"),
    )

@router.get("/trips")
def get_trips():
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        cur.execute("SELECT trip_id, route_id, service_id, trip_headsign, direction_id FROM trips;")
        rows = cur.fetchall()
        trips = []
        for row in rows:
            trips.append({
                "trip_id": row[0],
                "route_id": row[1],
                "service_id": row[2],
                "trip_headsign": row[3],
                "direction_id": row[4]
            })
        return trips
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        cur.close()
        conn.close()

=== FILE: api/routes/vehicle_positions.py ===
# api/routes/vehicle_positions.py
from fastapi import APIRouter, HTTPException
import psycopg2
import os
from datetime import datetime

router = APIRouter()

def get_db_connection():
    return psycopg2.connect(
        host=os.getenv("PGHOST", "db"),
        port=os.getenv("PGPORT", "5432"),
        dbname=os.getenv("PGDATABASE", "hslbussit"),
        user=os.getenv("PGUSER", "postgres"),
        password=os.getenv("PGPASSWORD", "supersecurepassword"),
    )

@router.get("/vehicle_positions")
def get_vehicle_positions():
    conn = get_db_connection()
    cur = conn.cursor()
    try:
        cur.execute("""
            SELECT vehicle_id, latitude, longitude, bearing, speed, timestamp
            FROM vehicle_positions
            ORDER BY timestamp DESC
            LIMIT 100;
        """)
        rows = cur.fetchall()
        positions = []
        for row in rows:
            positions.append({
                "vehicle_id": row[0],
                "latitude": row[1],
                "longitude": row[2],
                "bearing": row[3],
                "speed": row[4],
                "timestamp": row[5].isoformat() if isinstance(row[5], datetime) else row[5]
            })
        return positions
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        cur.close()
        conn.close()

=== FILE: api/routes/vehicles.py ===
import os
import psycopg2
from fastapi import APIRouter

router = APIRouter()

DB_HOST = os.getenv("PGHOST", "db")
DB_PORT = os.getenv("PGPORT", "5432")
DB_NAME = os.getenv("PGDATABASE", "hslbussit")
DB_USER = os.getenv("PGUSER", "postgres")
DB_PASS = os.getenv("PGPASSWORD", "supersecurepassword")

@router.get("/vehicles")
def get_vehicles():
    conn = psycopg2.connect(
        host=DB_HOST, port=DB_PORT, dbname=DB_NAME,
        user=DB_USER, password=DB_PASS
    )
    cur = conn.cursor()
    cur.execute("""
    SELECT
      veh       AS vehicle_id,
      desi      AS label,
      lat,
      long      AS lon,
      spd       AS speed,
      tst       AS timestamp
    FROM (
      SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY veh ORDER BY tst DESC) AS rn
      FROM mqtt_hfp
      WHERE tst > now() - interval '10 minutes'
    ) sub
    WHERE rn = 1;
    """)
    rows = cur.fetchall()
    cols = [desc[0] for desc in cur.description]
    result = [dict(zip(cols, row)) for row in rows]
    cur.close()
    conn.close()
    return result

=== FILE: backup.sh ===
#!/bin/sh

# Date format for backup file name
DATE=$(date +"%Y%m%d-%H%M%S")

# Full PostgreSQL data directory backup
tar czvf /backups/pgdata-backup-$DATE.tar.gz -C /var/lib/postgresql/data .

# Optional: keep only last 7 backups
cd /backups
ls -1tr | head -n -7 | xargs rm -f --

echo "Backup completed at $DATE"

=== FILE: create_snapshot.sh ===
#!/bin/bash

OUTPUT="files_snapshot.txt"
MAX_BYTES=5000

# Collect file list first
FILE_LIST=$(git ls-files | grep -v "^${OUTPUT}$" | sort)

# Write header and file index
{
  echo "# Project Snapshot: Bussikartta"
  echo "# Git Commit: $(git rev-parse HEAD)"
  echo "# Branch: $(git rev-parse --abbrev-ref HEAD)"
  echo "# Generated: $(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  echo ""
  echo "# Files in this snapshot:"
  echo "$FILE_LIST"
  echo ""
} > "$OUTPUT"

# Output content for each file
echo "$FILE_LIST" | while read -r file; do
  echo "=== FILE: $file ===" >> "$OUTPUT"
  if [ -f "$file" ]; then
    head -c "$MAX_BYTES" "$file" >> "$OUTPUT"
    FILE_SIZE=$(wc -c < "$file")
    if [ "$FILE_SIZE" -gt "$MAX_BYTES" ]; then
      echo -e "\n[... TRUNCATED]\n" >> "$OUTPUT"
    else
      echo -e "\n" >> "$OUTPUT"
    fi
  else
    echo "[SKIPPED: not a regular file]" >> "$OUTPUT"
    echo -e "\n" >> "$OUTPUT"
  fi
done

# Append ignored files (optional insight)
{
  echo -e "\n# Ignored Files:"
  git status --ignored --short | grep '^!!' || echo "(none)"
} >> "$OUTPUT"

=== FILE: docker-compose.yaml ===
version: "3.9"

services:
  api-server:
    build: .
    ports:
      - "8007:5000"
    environment:
      - PGHOST=db
      - PGPORT=5432
      - PGDATABASE=hslbussit
      - PGUSER=postgres
      - PGPASSWORD=supersecurepassword
    depends_on:
      - db
    volumes:
      - .:/app
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  db:
    image: timescale/timescaledb:2.15.2-pg15
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - timescale-data:/var/lib/postgresql/data
    ports:
      - "15432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  backup:
    image: alpine
    volumes:
      - .:/repo
      - ./backups:/backups
      - timescale-data:/var/lib/postgresql/data
    entrypoint: ["/bin/sh", "-c", "while true; do /repo/backup.sh; sleep 86400; done"]
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  gtfs-static:
    build:
      context: ./gtfs_static
    container_name: gtfs-static
    environment:
      - DB_HOST=db
      - DB_PORT=5432
      - DB_NAME=hslbussit
      - DB_USER=postgres
      - DB_PASS=supersecurepassword
    depends_on:
      - db
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  mqtt-ingest:
    build:
      context: ./ingestion/mqtt_hfp_ingest
    container_name: mqtt-ingest
    restart: unless-stopped
    environment:
      - DB_HOST=db
      - DB_PORT=5432
      - DB_NAME=hslbussit
      - DB_USER=postgres
      - DB_PASS=supersecurepassword
    depends_on:
      - db
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"
    volumes:
      - ./ingestion/mqtt_hfp_ingest:/app
      - /var/log:/var/log

  vehicle-ingest:
    build:
      context: .
    environment:
      - PYTHONUNBUFFERED=1
    working_dir: /app/ingestion
    command: python vehicle_positions_ingest.py
    depends_on:
      - db
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

volumes:
  timescale-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /volume1/docker/hslbussit/repo/dbdata

=== FILE: docker-compose.yaml.bak ===
version: "3.9"

services:
  api-server:
    build: .
    ports:
      - "8007:5000"
    environment:
      - DB_HOST=db
      - DB_PORT=5432
      - DB_NAME=${POSTGRES_DB}
      - DB_USER=${POSTGRES_USER}
      - DB_PASS=${POSTGRES_PASSWORD}
    depends_on:
      - db
    # Optional: live code reload during development
    volumes:
      - .:/app

  db:
    image: timescale/timescaledb:2.15.2-pg15
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - timescale-data:/var/lib/postgresql/data
    ports:
      - "15432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5

  backup:
    image: alpine
    volumes:
      - .:/repo
      - ./backups:/backups
      - timescale-data:/var/lib/postgresql/data
    entrypoint: ["/bin/sh", "-c", "while true; do /repo/backup.sh; sleep 86400; done"]

volumes:
  timescale-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /volume1/docker/hslbussit/repo/dbdata

=== FILE: Dockerfile ===
FROM python:3.12

WORKDIR /app

COPY api/requirements.txt .

RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "5000", "--reload"]

=== FILE: docs/AI-Guidelines.md ===
You are a full-stack AI engineer — part architect, part DevOps, part documentation-driven engineer.

**Your mission:**  
- memorize the rules
- wait for user to give details on the current project.

You own the entire development cycle. The user runs your commands, pastes your code, and gives you feedback. You write everything.

Stick to the following rules — no exceptions.

---

## 🔐 Output & Code Delivery

- Always deliver full, drop-in file replacements. No partials. No diffs.
- Output must be:
  - Executable as-is
  - Production-ready for **DSM 7.2.2 on Synology DS923+**
  - Pasteable by the user without edits
- When creating new files, use:
  \`\`\`bash
  cat <<EOF > /absolute/path/to/file.ext
  # your content here
  EOF
  \`\`\`
- If output is too large, provide a `curl` download link or alternate method.
- **Every major change must include a version note or changelog summary.**

---

## 🧠 AI Responsibilities

You are the engineer. You:
- Design the solution
- Read existing files
- Make verified changes
- Never guess, assume, or “try”
- Never suggest edits — **you provide exact edits as authoritative replacements**
- Always inspect the file before modifying it

Never tell the user to write or change code. You write it, they paste it.

---

## 🤝 Human-AI Workflow

The user:
- Runs commands
- Pastes full code
- Provides output or file contents when asked

You:
- Own the codebase
- Write all changes
- Validate any feedback

> ⚠️ The user never edits code manually. All changes — even small ones — go through you.

---

### 📎 Clarification: User Input & Code Authorship

If the user says:
> “Port 3000 is wrong — it should be 8080.”

You:
- Verify it
- Update the code
- Output the corrected file

Users can flag values, behaviors, or results — but never write code. You own authorship, context, and consistency.

---

## 💻 Terminal & Editor Usage

- **Terminal Editor:** User uses `vi`. No `nano`, no GUI editors.
- **Desktop Editor:** User uses **BBEdit**. You assume full visibility of open files.
- Always specify:
  - Full absolute paths for files, folders, and commands
  - File names and directory context

### Terminal Command Rules:
- No inline comments
- Prefer single-line commands unless multiline is necessary
- Limit large output to 10–30 lines using `grep`, `head`, `tail`
- Always include service/Docker restart commands where applicable

---

## ⚙️ Shell Aliases & Scripts

- Wrap repetitive CLI patterns in aliases:
  - Append to `~/.profile` using `echo >>`
  - Source the file immediately
- Prefer aliases over full scripts unless the logic is complex
- Always check with `alias` before redefining

---

## 🧭 Task & Context Management

- Track the current **Active Task** (e.g., `Active Task: Configure logging middleware`)
- If switching context for debugging:
  - Mark a **Temporary Task**
  - Restore the Active Task after it’s done
- If unsure about:
  - A file’s content
  - Project structure
  - Syntax or config state  
→ Pause and ask the user for specific output (e.g., `cat`, `ls`, `grep`)

> ⚠️ Don’t loop on broken solutions — debug with logs and inspection tools.  
> If standard debugging fails, escalate:  
> - Ask for broader logs or context  
> - Recommend a safe rollback or recovery command set

---

## 📊 Logging & Debugging Standards

All systems must include:
- **Backend logs** (services, APIs, middleware)
- **Frontend logs** (console, network inspector)
- **Infrastructure logs** (Docker, services)

Log output must:
- Be timestamped
- Use log levels (`debug`, `info`, `warn`, `error`)
- Be readable and actionable

Other requirements:
- Logging must be on by default
- Use log rotation and retention limits
- Minimize noise using filters or groupings
- **Never include credentials, secrets, or sensitive tokens in logs or code output.**
- **Sensitive data should be handled using environment variables, secrets managers, or secured DSM vault features.**

---

## 📋 Operational Expectations

Each major response must include:
- What just happened or what’s next
- Exact files and directories touched
- Task status (e.g., `Step 4 of 7`, `v1.1.2-subtask-b`)

You must remember:
- System constraints (DSM 7.2.2, DS923+ hardware)
- Project structure and history
- Logical effects of prior changes

Refresh your internal state periodically to avoid drift.

---

## 🧾 Enforcement

These rules are your contract.  
No exceptions unless explicitly versioned.

**This protocol overrides all informal habits and assumptions. It is the single source of truth.**

=== FILE: docs/developer_handbook.md ===
# Developer Handbook

## 📦 System Overview

This project is a **real-time bus tracking system** for Helsinki Region Transport (HSL), capable of rendering thousands of live vehicle positions on a blazing fast vector tile map.

---

## 🧭 Architecture Summary

- **Backend**: FastAPI-based Python service exposing REST & WebSocket
- **Frontend**: SolidJS + Vite with MapLibre GL JS
- **Map Tiles**: HSL vector tiles (online) or Tileserver-GL (offline)
- **Database**: TimescaleDB (PostgreSQL)
- **Deployment**: Full Docker setup
- **Data**: Static GTFS + MQTT-based real-time vehicle feeds

---

## 🚀 Getting Started

### ✅ 1. Clone the repository

```bash
git clone https://github.com/YOUR_ORG/bussikartta.git
cd bussikartta
```

### ✅ 2. Launch the stack

```bash
docker compose up -d
```

Then access:
- Backend REST: http://localhost:8007/vehicles
- WebSocket: ws://localhost:8007/ws
- (If configured) Tile Server: http://localhost:8080
- Frontend Dev Server: http://localhost:5173

---

## 🧱 Folder Structure (Key)

```
repo/
├── api/                  # FastAPI backend
├── ingestion/            # MQTT + GTFS-RT ingestors
├── gtfs_static/          # Static GTFS importer
├── docs/                 # Architecture and dev docs
├── tools/                # Watchdog scripts
├── Dockerfile            # Backend container
├── docker-compose.yaml   # Services & ports
└── init_timescale.sql    # DB schema
```

---

## 🧩 Frontend Overview (SolidJS)

| Element       | Stack/Choice           |
| ------------- | ---------------------- |
| Framework     | SolidJS                |
| Dev Server    | Vite (`npm run dev`)   |
| Map Engine    | MapLibre GL JS         |
| Updates       | WebSocket              |
| Volume        | Supports 5,000+ markers|

Frontend connects to backend via:
- `fetch` (initial load)
- `WebSocket` (live updates, every 1s)
- Offline tiles supported via `.mbtiles` if Tileserver-GL is running.

---

## 🛰 Backend API

| Method | Path         | Description                        |
|--------|--------------|------------------------------------|
| GET    | `/vehicles`  | Current live vehicle positions     |
| WS     | `/ws`        | Streams 1s real-time JSON objects  |

### JSON Format:

```json
{
  "vehicle_id": "1234",
  "label": "600N",
  "lat": 60.17,
  "lon": 24.94,
  "speed": 33.0,
  "timestamp": "2025-06-19T00:00:00Z"
}
```

---

## 🛠 Watchdog & Cron Tools

Located in `tools/`:

- `mqtt_watchdog.sh`: Verifies live MQTT ingestion is active
- `vehicle_watchdog.sh`: Checks if recent vehicles exist in DB

Example usage via crontab or log-based monitoring.

---

## 📄 Reference Documents

| File                                | Purpose                                |
|-------------------------------------|----------------------------------------|
| `docs/project_architecture_plan.md` | Locked architectural decisions         |
| `docs/frontend_architecture.md`     | Map and component logic                |
| `docs/services.md`                  | Ports and service overview             |
| `docs/gtfs_data_handling.md`        | Static transit schedule import process |

---

## 📚 GTFS Data

### Static Feed
- URL: https://infopalvelut.storage.hsldev.com/gtfs/hsl.zip
- Parser: `gtfs_static/main.py`
- Imports `stops`, `routes`, `trips`, `calendar`, etc.

### Realtime
- MQTT Broker: `mqtt.hsl.fi` (port 1883 or 8883)
- Topic: `/hfp/v2/journey/#`
- Listener: `ingestion/mqtt_hfp_ingest/main.py`
- Output Table: `mqtt_hfp`

---

## 🗺 Map Tiles

| Mode    | Details                                  |
|---------|------------------------------------------|
| Online  | `cdn.digitransit.fi/hsl-vector-map`      |
| Offline | Tilemaker (`.mbtiles`) + Tileserver-GL   |

To use offline mode, generate `.mbtiles` from `hsl.osm.pbf`, mount it in the container, and run a vector tile server.

---

## 🧼 Final Notes

- Backend written in idiomatic FastAPI, one route file per GTFS domain
- System built for high-concurrency WebSocket performance
- Fully portable and containerized for deployment

_Last updated: 2025-06-18T23:44:39.748611Z_


=== FILE: docs/frontend_architecture.md ===
# Frontend Architecture (Updated)

This document defines the architecture for the **Bussikartta** frontend, designed for blazing-fast real-time map rendering of 5,000+ vehicles.

---

## 🧱 Technology Stack

| Technology      | Purpose                                              |
|------------------|------------------------------------------------------|
| SolidJS         | Core UI framework (ultra-fast, minimal re-renders)   |
| Vite            | Build tool + dev server (`npm run dev`)              |
| MapLibre GL JS  | WebGL-based map rendering engine                     |
| TypeScript      | Ensures safety and structure in the codebase         |
| WebSocket       | Push-based live updates every 1 second               |
| Docker (optional) | For static build + container deployment            |

---

## 🧠 Architecture Summary

- **Map rendering** is handled by **MapLibre GL JS**, fed with GeoJSON sources.
- **Live updates** are pushed from the backend WebSocket `/ws` and injected into the map's data source using `setData()`.
- The app avoids unnecessary DOM elements — vehicle markers are rendered by the map engine itself.
- Core state (vehicle data) is reactive and drives UI updates.

---

## 🔧 Component Breakdown

| Component             | Role                                                          |
|-----------------------|---------------------------------------------------------------|
| `<MapView>`           | Initializes MapLibre map and loads vehicle markers            |
| `<VehiclePopup>`      | (Optional) Shows label + speed for selected vehicle           |
| `<Controls>`          | Filter, zoom, route selector (optional UI)                    |
| `<Header>`/`<Footer>` | Branding, data source, timestamps                             |

---

## 🗺 Map Tile Source

- **Online**: HSL vector tile service (`hsl-vector-map`)
- **Offline**: `.mbtiles` hosted locally via **Tileserver GL**

---

## 🔄 Data Flow

```mermaid
sequenceDiagram
    participant Server as Backend (FastAPI)
    participant Client as SolidJS Frontend
    participant Map as MapLibre Map

    Server-->>Client: WebSocket JSON (every 1s)
    Client->>Map: Replace vehicle GeoJSON via setData()
    Client->>UI: (Optional) update timestamp, info overlays
```

---

## 🧪 Dev Notes

- Run `npm run dev` in the frontend folder (powered by Vite).
- Open browser to `http://localhost:5173`
- Ensure backend WebSocket is reachable on `/ws` (e.g. port 8007).
- Tile style can be switched using standard MapLibre `style` config.
- GeoJSON source must be promoted by ID to allow efficient layer updates.

---

## ✅ Locked Technologies

- SolidJS + Vite
- MapLibre GL JS
- WebSocket (no polling, no React Query)
- Vector tiles (HSL online or offline via mbtiles)

---

© HSL Bussikartta 2025


=== FILE: docs/gtfs_data_handling.md ===
# GTFS Data Handling

This document explains how Bussikartta handles **GTFS (General Transit Feed Specification)** data, specifically the static transit schedule data. It covers how GTFS files are obtained, parsed, and imported into the system, how updates to the data are managed, and how the static data is used in conjunction with real-time information. Understanding GTFS handling is crucial, as it provides the foundational context (routes, stops, timetables) that make the real-time vehicle data meaningful.

## What is GTFS and Why It Matters

**GTFS** is a standard format for public transit schedules and associated geographic information【5†L95-L103】. Transit agencies (like HSL in Helsinki) publish their route schedules, stop locations, and other transit data as a GTFS package (usually a ZIP file containing multiple text files). GTFS is split into:
- **GTFS Static** (schedules): includes routes, trips, stops, stop times, calendars, etc.
- **GTFS Realtime**: a separate feed (often using protocol buffers) for live updates like vehicle positions or delays.

Bussikartta uses the GTFS static data to know **where and when vehicles are supposed to be**:
- By loading route and stop definitions, the system can display route names and stop names instead of IDs.
- By loading the timetable (trips and stop times), it can compare a bus’s actual timing to the schedule and compute delays or detect if a vehicle is off-schedule.
- Essentially, GTFS static data provides the *planned world* against which the *real-time world* is measured.

## Importing GTFS Static Files

**Source of GTFS Data:** For the Helsinki region, HSL provides a GTFS feed that is updated daily【27†L65-L72】. The latest GTFS static zip can be downloaded from HSL’s open data website (a link is typically provided, e.g., to an HSL data storage URL). Other regions (like Tampere’s TKL or Waltti cities) also provide GTFS feeds, either through separate URLs or an API (often requiring a key).

In Bussikartta, the GTFS static import process is as follows:

1. **Download the GTFS Zip:** The system either includes a script to fetch the file from a configured URL or expects the user to provide the GTFS zip file. For HSL, the daily-updated zip is accessible at a known URL【27†L65-L72】 (for example, `https://infopalvelut.storage.hsldev.com/gtfs/hsl.zip` – actual link may differ, but that’s the idea). In deployment, one can automate downloading this file periodically.
   
2. **Unzip and Parse CSV Files:** GTFS zip contains multiple text files (CSV format, usually `,` or `,` separated). Key files we parse:
   - `agency.txt` – transit agency info (not critical for our purposes except for reference).
   - `stops.txt` – list of all stops and their coordinates.
   - `routes.txt` – list of all routes (each route has an ID, short name, long name, type like bus or tram, etc.).
   - `trips.txt` – list of all trips. Each trip is an instance of a route on a specific service (e.g., a bus journey for a given day), with a trip ID, and references to route_id and service_id (which days it runs).
   - `stop_times.txt` – schedule times for each trip at each stop (sequence of stops with arrival/departure times).
   - `calendar.txt` and `calendar_dates.txt` – define the service calendar (which days the regular schedules operate, and exceptions/holidays). Important for figuring out which trips are active on a given date.
   - `shapes.txt` – (optional) geographic path for trips (sequence of lat/lon points). Useful for drawing route lines on a map.

   The import code reads these files (commonly using a CSV reader or a specialized GTFS library). Because these files can be large (HSL has tens of thousands of stop_times rows), the parsing is done carefully (streaming line by line, or using bulk copy to database for efficiency).
   
3. **Database Import:** After parsing, the data is inserted into the database tables:
   - **Stops table:** Each record from stops.txt becomes a row (stop_id, name, latitude, longitude, etc.).
   - **Routes table:** Each record from routes.txt becomes a row (route_id, short_name, long_name, type, etc.).
   - **Trips table:** Each record from trips.txt becomes a row (trip_id, route_id, service_id, headsign, direction, etc.).
   - **StopTimes table:** Each record from stop_times.txt becomes a row (trip_id, stop_id, arrival_time, departure_time, stop_sequence, etc.).
   - **Calendar/CalendarDates tables:** These can be loaded to determine service validity. (Alternatively, some systems merge this logic into filtering trips by date when needed rather than storing full calendar info.)
   - **Shapes table:** If used, each shape_id’s polyline points could be stored, or perhaps generated on the fly. (Storing shapes might be skipped if not needed immediately, to save space.)

   We ensure appropriate indexing on these tables. For example:
   - Index on `trip_id` in StopTimes (since queries will often filter by trip).
   - Composite ind
[... TRUNCATED]

=== FILE: docs/overview.md ===
# Bussikartta Overview

## 1. Basic Overview

**Bussikartta** is a real-time transit tracking system designed for Helsinki Region Transport (HSL). It combines static schedule data with live vehicle telemetry to show real-time positions and delay information of buses on an interactive map.

### Key Functionalities

- **Static GTFS Schedule Import**: Imports routes, trips, stops, and timetables from HSL’s GTFS feed into a TimescaleDB database.
- **Live Data Ingestion**: Subscribes to HSL’s MQTT feed to receive live vehicle position updates and status events.
- **Delay Calculation**: Computes delays (ahead or behind schedule) by comparing live arrival times to scheduled times.
- **API for Clients**: Exposes REST and WebSocket endpoints via FastAPI for querying vehicles, delays, stops, and routes.
- **Interactive Frontend**: A SolidJS + Vite-based web app that displays bus positions and delays on an interactive vector tile map.
- **Containerized Setup**: Uses Docker Compose to orchestrate backend, frontend, tile server, and database services.

---

## 2. Detailed Overview

### 🏛 Architecture

The system comprises several coordinated services:

```
[ GTFS Feed ] ─── import_gtfs.py ──► [TimescaleDB Static GTFS Tables]
                   ↑
                   └─────────────────┐
                                     ▼
                                FastAPI Backend ◄─────────────
                   ▲           (REST APIs, WebSocket `/ws`)   │
                   │                                           │
[ HSL MQTT        └───────────────────────────────────────────►│
  Live Feed ]           MQTT Subscriber        Live Vehicle Data│
                                                         ▲     │
                                                         └─────┘
                                       SolidJS + MapLibre Frontend (WebSocket)
```

---

### Core Components

#### 1. `gtfs_static/main.py`
- Downloads and parses the GTFS ZIP feed from HSL.
- Populates static database tables: `routes`, `trips`, `stop_times`, `stops`, etc.

#### 2. `mqtt_hfp_ingest/main.py`
- Subscribes to the HSL MQTT broker (topic `/hfp/v2/journey/ongoing/vp/bus/#`).
- On each message, inserts vehicle telemetry into the `mqtt_hfp` hypertable.

#### 3. `api/` (FastAPI backend)
- Serves:
  - `/vehicles` (REST) — snapshot of latest positions.
  - `/ws` (WebSocket) — streams JSON vehicle updates every second.
- Responsible for:
  - Joining static and dynamic data.
  - Delivering low-latency access to vehicle feeds.

#### 4. `frontend/`
- Powered by **SolidJS + Vite**.
- Uses **MapLibre GL JS** to render fast vector tiles from:
  - HSL CDN (`hsl-vector-map`) or
  - Local `.mbtiles` via Tileserver-GL (optional).
- WebSocket-driven updates allow rendering 5,000+ live markers efficiently.

#### 5. `tileserver/` (optional)
- Dockerized **Tileserver-GL** instance.
- Serves `.mbtiles` generated by **Tilemaker** for offline use.

#### 6. `docker-compose.yml`
- Defines all services:
  - `api-server`, `db`, `mqtt-ingest`, `gtfs-static`, `bussikartta-ui`, `bussikartta-map`.
- Docker volumes are used for database persistence and optional tiles.

---

### 💡 Developer Highlights

- **WebSocket-first updates** — frontend connects to `/ws`, receives 1 Hz updates.
- **No React** — SolidJS ensures fast, minimal re-renders for map-heavy UIs.
- **Map tile options** — Online from CDN or offline from local `.mbtiles`.
- **Clear data flow** — Static (GTFS) and dynamic (MQTT) data merged in backend.

---

© HSL Bussikartta 2025


=== FILE: docs/project_architecture.md ===
# Project Architecture

This document provides an in-depth overview of Bussikartta’s backend architecture, covering its components, data flow, database design, and the principles behind its design. The focus is on how the system ingests data (both static and real-time), how it stores and organizes that data in TimescaleDB, and how the FastAPI backend serves it via a well-defined API.

## System Overview

Bussikartta’s backend is structured as a set of cooperating components, each with a clear role:

- **Data Ingestion Pipeline:** Responsible for fetching and processing data from external sources. This includes **GTFS static data ingestion** (loading schedules and reference data into the database) and **real-time data ingestion** (subscribing to live vehicle position updates via MQTT).
- **Core Database (TimescaleDB):** A PostgreSQL-based database optimized for time-series. It stores both static transit data (routes, stops, timetables) and dynamic data (vehicle positions over time, updates). Using TimescaleDB’s hypertables allows efficient storage and querying of large sequences of timestamped location data.
- **FastAPI Application (Backend API):** A Python web API that exposes endpoints for clients (including the frontend). It handles incoming HTTP requests, queries the database as needed, and returns JSON responses. It also contains any business logic (e.g., computing delays by comparing real-time data with static schedules).
- **MQTT Client (Real-time Subscriber):** A background task or separate module in the backend that maintains a connection to the MQTT broker broadcasting vehicle positions. It listens for messages on specific topics and processes each message (e.g., parsing the vehicle’s data and inserting a new record into the database).
- **Planned Frontend (React App):** Although primarily a consumer of the API (and documented separately), it’s part of the overall architecture. The frontend makes requests to the FastAPI backend to retrieve current vehicle positions, route info, etc., and visualizes them. In the future, a WebSocket or similar push mechanism might be added to stream updates to the frontend.

Below is a high-level diagram of how these components interact:

```mermaid
flowchart LR
    subgraph External Sources
        A[GTFS Static Feed<br>(Schedule Data)]
        B[HSL MQTT Broker<br>(Real-time Data)]
    end
    subgraph Bussikartta Backend
        C[GTFS Ingestion<br>Module]
        D[MQTT Subscriber<br>Module]
        E[(TimescaleDB)]
        F[FastAPI REST API]
    end
    subgraph Frontend Client
        G[React Map UI]
    end

    A -- GTFS ZIP --> C
    B -- Live messages --> D
    C -- Schedule records --> E
    D -- Vehicle position inserts --> E
    F -- Read/Write --> E
    G -- API calls --> F
    F -- JSON data --> G
```

**Figure: System architecture and data flow.** Solid arrows indicate data flow: GTFS static data is downloaded by the ingestion module and stored in the database; real-time messages stream from the HSL MQTT broker to our subscriber which writes them to the database. The FastAPI API reads from (and occasionally writes to) the database, serving client requests. The frontend interacts with the system purely via the API.

## Components and Responsibilities

### FastAPI Backend (API Server)
The FastAPI application is the central piece that clients interact with. It provides organized endpoints (RESTful routes) to retrieve information like current vehicles, routes, stops, etc. Key characteristics of the API server:
- **Routing & Logic:** The code is likely organized into routers or controllers by domain (vehicles, routes, stops). Each endpoint handler will query the database (using an async database client or an ORM/SQL) and assemble the result.
- **Pydantic Models:** FastAPI uses Pydantic for defining request/response data models. The API likely defines schemas (for example, a `Vehicle` model with fields like id, latitude, longitude, route, delay, etc.) to ensure consistent output.
- **Async I/O:** FastAPI supports asynchronous operation. Database queries and MQTT handling can run asynchronously so that the server remains responsive under load. This is important given potentially high frequency of incoming data.
- **Background Tasks:** The API server can spawn background tasks. Bussikartta might use this for the MQTT subscription – for example, on startup, launch a background task to connect to the broker and process messages continually. Alternately, the MQTT ingestion could run as a separate process/service (depending on design).
- **Auto-Documentation:** By leveraging FastAPI, the backend automatically provides an OpenAPI schema and interactive docs at `/docs`. This is useful for developers to explore the available endpoints.

### Data Ingestion Pipeline
The ingestion pipeline has two major parts: **static data ingestion** and **real-time data ingestion**.

- **GTFS Static Ingestion:** This is typically a batch process triggered manually or on
[... TRUNCATED]

=== FILE: docs/project_architecture_plan.md ===
# Bussikartta Architecture Plan v1.1

## 1. 🛍 Frontend

| Element         | Choice                                                |
| --------------- | ----------------------------------------------------- |
| Framework       | SolidJS + Vite                                        |
| Map engine      | MapLibre GL JS                                        |
| Live updates    | WebSocket (1-second interval)                         |
| Marker volume   | Supports 5,000+ real-time vehicle markers             |
| Labels          | Vehicle label + speed overlay (e.g. "600N @ 34 km/h") |
| UI Enhancements | Optional: timestamp, clustering, trails               |
| Dev mode        | `npm run dev` via Vite                                |
| Deployment      | Docker or static build via NGINX                      |

## 2. 🚀 Backend

| Element            | Choice                                                  |
| ------------------ | ------------------------------------------------------- |
| Framework          | Python (current) with upgrade path to FastAPI           |
| Live transport     | WebSocket (primary), Polling (fallback)                 |
| Data format        | JSON: `{vehicle_id, label, lat, lon, speed, timestamp}` |
| WebSocket endpoint | `/ws`                                                   |
| Containerization   | Yes (e.g. `repo-api-server-1` on port `8007`)           |

## 3. 🗌 Map Tiles

| Element             | Choice                                                         |
| ------------------- | -------------------------------------------------------------- |
| Online base map     | HSL Vector Tiles (`hsl-vector-map`)                            |
| Offline fallback    | Tilemaker for `.pbf` vector tiles                              |
| Offline tile server | Tileserver GL with local `.mbtiles`                            |
| Style compatibility | HSL / OpenMapTiles / MapLibre-ready                            |
| Deployment          | Docker container with mounted tile volume                      |
| OSM source          | `https://karttapalvelu.storage.hsldev.com/hsl.osm/hsl.osm.pbf` |

## 4. 🔌 Infrastructure

| Element              | Choice                                 |
| -------------------- | -------------------------------------- |
| Containerization     | Full Docker deployment                 |
| Reverse proxy        | Optional: NGINX / Traefik              |
| Data update interval | Offline tiles: manual, Vehicles: live  |
| Logging & Monitoring | Optional: container logs, dev overlays |

## 🔒 Locked Decisions

* SolidJS + Vite frontend
* MapLibre GL JS
* WebSocket for real-time transport
* Offline vector tiles via Tilemaker
* Vector tile hosting via Tileserver-GL
* Backend maintained and included in architecture
* Fully Dockerized infrastructure

=== FILE: docs/services.md ===
# Bussikartta Services (v1.1)

## 🧭 Core Services

| Service      | Type     | Port | Description                             |
| ------------ | -------- | ---- | --------------------------------------- |
| Frontend     | Vite Dev | 5173 | Development server                      |
| Frontend     | NGINX    | 80   | Static production deployment            |
| Backend      | Python   | 8007 | REST `/vehicles`, WebSocket `/ws`       |
| Map Tile API | HTTP     | 8080 | (Optional) Tileserver GL for `.mbtiles` |

---

## 🔌 External Dependencies

| Source         | Use                         |
| -------------- | --------------------------- |
| HSL Vector Map | `cdn.digitransit.fi`        |
| GTFS Static    | Manual `.zip`               |
| GTFS-RT        | Streamed by backend         |
| OSM PBF        | `hsl.osm.pbf` for Tilemaker |

---

## 🗺 Map Tile Service (Offline)

| Component     | Technology    | Details                             |
| ------------- | ------------- | ----------------------------------- |
| Generator     | Tilemaker     | Converts `.osm.pbf` to `.mbtiles`   |
| Tile Server   | Tileserver GL | Serves vector tiles from `.mbtiles` |
| Containerized | Docker        | Mounted local tile volume           |

---

## 🧱 WebSocket Details

| Endpoint  | `/ws`              |
| --------- | ------------------ |
| Format    | JSON per vehicle   |
| Frequency | Every second       |

```json
{
  "vehicle_id": "1234",
  "label": "600N",
  "lat": 60.17,
  "lon": 24.94,
  "speed": 33.0,
  "timestamp": "2025-06-19T00:00:00Z"
}
🐳 Container Names (Example)


=== FILE: feed.pb ===
This endpoint has been deprecated and removed. More information available at https://digitransit.fi/en/developers/deprecations/

=== FILE: .gitignore ===
__pycache__/
*.pyc
*.pyo
*.pyd
*.db
*.sqlite
*.log
.DS_Store
api/.DS_Store
.env
dbdata/
backups/
@eaDir/


=== FILE: gtfs_static/Dockerfile ===
FROM python:3.12-slim

RUN apt-get update && apt-get install -y gcc libpq-dev && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY . .

RUN pip install --no-cache-dir -r requirements.txt

CMD ["python", "main.py"]


=== FILE: gtfs_static/main.py ===
import os
import zipfile
import requests
import psycopg2
import pandas as pd
from io import BytesIO

GTFS_URL = "https://infopalvelut.storage.hsldev.com/gtfs/hsl.zip"

DB_HOST = os.getenv("DB_HOST", "db")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "hslbussit")
DB_USER = os.getenv("DB_USER", "postgres")
DB_PASS = os.getenv("DB_PASS", "supersecurepassword")

def get_db_connection():
    return psycopg2.connect(
        host=DB_HOST, port=DB_PORT, dbname=DB_NAME,
        user=DB_USER, password=DB_PASS
    )

def download_gtfs():
    print("Downloading GTFS...")
    r = requests.get(GTFS_URL)
    r.raise_for_status()
    return BytesIO(r.content)

def extract_gtfs(zip_data):
    print("Extracting files...")
    with zipfile.ZipFile(zip_data) as z:
        z.extractall("/tmp/gtfs_static")

def create_tables():
    with get_db_connection() as conn:
        cur = conn.cursor()
        cur.execute("""
            CREATE TABLE IF NOT EXISTS agency (
                agency_id TEXT PRIMARY KEY,
                agency_name TEXT,
                agency_url TEXT,
                agency_timezone TEXT
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS stops (
                stop_id TEXT PRIMARY KEY,
                stop_name TEXT,
                stop_lat DOUBLE PRECISION,
                stop_lon DOUBLE PRECISION
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS routes (
                route_id TEXT PRIMARY KEY,
                route_short_name TEXT,
                route_long_name TEXT,
                route_type INTEGER
            );
        """)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS trips (
                trip_id TEXT PRIMARY KEY,
                route_id TEXT,
                service_id TEXT,
                trip_headsign TEXT,
                direction_id INTEGER
            );
        """)
        conn.commit()

def load_data():
    with get_db_connection() as conn:
        cur = conn.cursor()

        agency = pd.read_csv("/tmp/gtfs_static/agency.txt")
        for _, row in agency.iterrows():
            cur.execute("""
                INSERT INTO agency (agency_id, agency_name, agency_url, agency_timezone)
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (agency_id) DO UPDATE SET agency_name=EXCLUDED.agency_name
            """, (row['agency_id'], row['agency_name'], row['agency_url'], row['agency_timezone']))

        stops = pd.read_csv("/tmp/gtfs_static/stops.txt")
        for _, row in stops.iterrows():
            cur.execute("""
                INSERT INTO stops (stop_id, stop_name, stop_lat, stop_lon)
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (stop_id) DO UPDATE SET stop_name=EXCLUDED.stop_name
            """, (row['stop_id'], row['stop_name'], row['stop_lat'], row['stop_lon']))

        routes = pd.read_csv("/tmp/gtfs_static/routes.txt")
        for _, row in routes.iterrows():
            cur.execute("""
                INSERT INTO routes (route_id, route_short_name, route_long_name, route_type)
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (route_id) DO UPDATE SET route_short_name=EXCLUDED.route_short_name
            """, (row['route_id'], row['route_short_name'], row['route_long_name'], row['route_type']))

        trips = pd.read_csv("/tmp/gtfs_static/trips.txt")
        for _, row in trips.iterrows():
            cur.execute("""
                INSERT INTO trips (trip_id, route_id, service_id, trip_headsign, direction_id)
                VALUES (%s, %s, %s, %s, %s)
                ON CONFLICT (trip_id) DO UPDATE SET trip_headsign=EXCLUDED.trip_headsign
            """, (row['trip_id'], row['route_id'], row['service_id'], row['trip_headsign'], row['direction_id']))

        conn.commit()

if __name__ == "__main__":
    zip_data = download_gtfs()
    extract_gtfs(zip_data)
    create_tables()
    load_data()
    print("✅ GTFS static import done.")


=== FILE: gtfs_static/requirements.txt ===
pandas
requests
psycopg2-binary


=== FILE: ingestion/config.py ===
import os

GTFS_VEHICLE_URL = "https://realtime.hsl.fi/realtime/vehicle-positions/v2/hsl"

DB_HOST = "db"
DB_PORT = 5432
DB_NAME = "hslbussit"
DB_USER = "postgres"
DB_PASS = "supersecurepassword"

=== FILE: ingestion/gtfs_realtime_pb2.py ===
# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: gtfs-realtime.proto
"""Generated protocol buffer code."""
from google.protobuf.internal import builder as _builder
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x13gtfs-realtime.proto\x12\x10transit_realtime\"y\n\x0b\x46\x65\x65\x64Message\x12,\n\x06header\x18\x01 \x02(\x0b\x32\x1c.transit_realtime.FeedHeader\x12,\n\x06\x65ntity\x18\x02 \x03(\x0b\x32\x1c.transit_realtime.FeedEntity*\x06\x08\xe8\x07\x10\xd0\x0f*\x06\x08\xa8\x46\x10\x90N\"\xed\x01\n\nFeedHeader\x12\x1d\n\x15gtfs_realtime_version\x18\x01 \x02(\t\x12Q\n\x0eincrementality\x18\x02 \x01(\x0e\x32+.transit_realtime.FeedHeader.Incrementality:\x0c\x46ULL_DATASET\x12\x11\n\ttimestamp\x18\x03 \x01(\x04\x12\x14\n\x0c\x66\x65\x65\x64_version\x18\x04 \x01(\t\"4\n\x0eIncrementality\x12\x10\n\x0c\x46ULL_DATASET\x10\x00\x12\x10\n\x0c\x44IFFERENTIAL\x10\x01*\x06\x08\xe8\x07\x10\xd0\x0f*\x06\x08\xa8\x46\x10\x90N\"\xe1\x02\n\nFeedEntity\x12\n\n\x02id\x18\x01 \x02(\t\x12\x19\n\nis_deleted\x18\x02 \x01(\x08:\x05\x66\x61lse\x12\x31\n\x0btrip_update\x18\x03 \x01(\x0b\x32\x1c.transit_realtime.TripUpdate\x12\x32\n\x07vehicle\x18\x04 \x01(\x0b\x32!.transit_realtime.VehiclePosition\x12&\n\x05\x61lert\x18\x05 \x01(\x0b\x32\x17.transit_realtime.Alert\x12&\n\x05shape\x18\x06 \x01(\x0b\x32\x17.transit_realtime.Shape\x12$\n\x04stop\x18\x07 \x01(\x0b\x32\x16.transit_realtime.Stop\x12?\n\x12trip_modifications\x18\x08 \x01(\x0b\x32#.transit_realtime.TripModifications*\x06\x08\xe8\x07\x10\xd0\x0f*\x06\x08\xa8\x46\x10\x90N\"\xf6\x0b\n\nTripUpdate\x12.\n\x04trip\x18\x01 \x02(\x0b\x32 .transit_realtime.TripDescriptor\x12\x34\n\x07vehicle\x18\x03 \x01(\x0b\x32#.transit_realtime.VehicleDescriptor\x12\x45\n\x10stop_time_update\x18\x02 \x03(\x0b\x32+.transit_realtime.TripUpdate.StopTimeUpdate\x12\x11\n\ttimestamp\x18\x04 \x01(\x04\x12\r\n\x05\x64\x65lay\x18\x05 \x01(\x05\x12\x44\n\x0ftrip_properties\x18\x06 \x01(\x0b\x32+.transit_realtime.TripUpdate.TripProperties\x1ai\n\rStopTimeEvent\x12\r\n\x05\x64\x65lay\x18\x01 \x01(\x05\x12\x0c\n\x04time\x18\x02 \x01(\x03\x12\x13\n\x0buncertainty\x18\x03 \x01(\x05\x12\x16\n\x0escheduled_time\x18\x04 \x01(\x03*\x06\x08\xe8\x07\x10\xd0\x0f*\x06\x08\xa8\x46\x10\x90N\x1a\xb9\x07\n\x0eStopTimeUpdate\x12\x15\n\rstop_sequence\x18\x01 \x01(\r\x12\x0f\n\x07stop_id\x18\x04 \x01(\t\x12;\n\x07\x61rrival\x18\x02 \x01(\x0b\x32*.transit_realtime.TripUpdate.StopTimeEvent\x12=\n\tdeparture\x18\x03 \x01(\x0b\x32*.transit_realtime.TripUpdate.StopTimeEvent\x12U\n\x1a\x64\x65parture_occupancy_status\x18\x07 \x01(\x0e\x32\x31.transit_realtime.VehiclePosition.OccupancyStatus\x12j\n\x15schedule_relationship\x18\x05 \x01(\x0e\x32@.transit_realtime.TripUpdate.StopTimeUpdate.ScheduleRelationship:\tSCHEDULED\x12\\\n\x14stop_time_properties\x18\x06 \x01(\x0b\x32>.transit_realtime.TripUpdate.StopTimeUpdate.StopTimeProperties\x1a\xff\x02\n\x12StopTimeProperties\x12\x18\n\x10\x61ssigned_stop_id\x18\x01 \x01(\t\x12\x15\n\rstop_headsign\x18\x02 \x01(\t\x12\x65\n\x0bpickup_type\x18\x03 \x01(\x0e\x32P.transit_realtime.TripUpdate.StopTimeUpdate.StopTimeProperties.DropOffPickupType\x12g\n\rdrop_off_type\x18\x04 \x01(\x0e\x32P.transit_realtime.TripUpdate.StopTimeUpdate.StopTimeProperties.DropOffPickupType\"X\n\x11\x44ropOffPickupType\x12\x0b\n\x07REGULAR\x10\x00\x12\x08\n\x04NONE\x10\x01\x12\x10\n\x0cPHONE_AGENCY\x10\x02\x12\x1a\n\x16\x43OORDINATE_WITH_DRIVER\x10\x03*\x06\x08\xe8\x07\x10\xd0\x0f*\x06\x08\xa8\x46\x10\x90N\"P\n\x14ScheduleRelationship\x12\r\n\tSCHEDULED\x10\x00\x12\x0b\n\x07SKIPPED\x10\x01\x12\x0b\n\x07NO_DATA\x10\x02\x12\x0f\n\x0bUNSCHEDULED\x10\x03*\x06\x08\xe8\x07\x10\xd0\x0f*\x06\x08\xa8\x46\x10\x90N\x1a\x9b\x01\n\x0eTripProperties\x12\x0f\n\x07trip_id\x18\x01 \x01(\t\x12\x12\n\nstart_date\x18\x02 \x01(\t\x12\x12\n\nstart_time\x18\x03 \x01(\t\x12\x10\n\x08shape_id\x18\x04 \x01(\t\x12\x15\n\rtrip_headsign\x18\x05 \x01(\t\x12\x17\n\x0ftrip_short_name\x18\x06 \x01(\t*\x06\x08\xe8\x07\x10\xd0\x0f*\x06\x08\xa8\x46\x10\x90N*\x06\x08\xe8\x07\x10\xd0\x0f*\x06\x08\xa8\x46\x10\x90N\"\xdf\t\n\x0fVehiclePosition\x12.\n\x04trip\x18\x01 \x01(\x0b\x32 .transit_realtime.TripDescriptor\x12\x34\n\x07vehicle\x18\x08 \x01(\x0b\x32#.transit_realtime.VehicleDescriptor\x12,\n\x08position\x18\x02 \x01(\x0b\x32\x1a.transit_realtime.Position\x12\x1d\n\x15\x63urrent_stop_sequence\x18\x03 \x01(\r\x12\x0f\n\x07stop_id\x18\x07 \x01(\t\x12Z\n\x0e\x63urrent_status\x18\x04 \x01(\x0e\x32\x33.transit_realtime.VehiclePosition.VehicleStopStatus:\rIN_TRANSIT_TO\x12\x11\n\ttimestamp\x18\x05 \x01(\x04\x12K\n\x10\x63ongestion_level\x18\x06 \x01(\x0e\x32\x31.transit_realtime.VehiclePosition.CongestionLevel\x12K\n\x10occupancy_status\x18\t \x01(
[... TRUNCATED]

=== FILE: ingestion/__init__.py ===


=== FILE: ingestion/mqtt_hfp_ingest/Dockerfile ===
FROM python:3.12-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY main.py .

CMD ["python", "main.py"]


=== FILE: ingestion/mqtt_hfp_ingest/main.py ===
import os
import json
import time
import logging
import psycopg2
import paho.mqtt.client as mqtt
from datetime import datetime

# Environment variables
MQTT_BROKER = os.getenv("MQTT_BROKER", "mqtt.hsl.fi")
MQTT_PORT = int(os.getenv("MQTT_PORT", "1883"))
MQTT_TOPIC = os.getenv("MQTT_TOPIC", "/hfp/v2/journey/ongoing/vp/bus/#")

DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "hslbussit")
DB_USER = os.getenv("DB_USER", "postgres")
DB_PASS = os.getenv("DB_PASS", "supersecurepassword")

# Setup logging
logfile_path = "/var/log/mqtt_ingest.log"
os.makedirs(os.path.dirname(logfile_path), exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(logfile_path),
        logging.StreamHandler()
    ]
)

def log(msg): logging.info(msg)

# MQTT event callbacks
def on_connect(client, userdata, flags, rc):
    log(f"Connected to MQTT broker {MQTT_BROKER}:{MQTT_PORT} with result code {rc}")
    client.subscribe(MQTT_TOPIC)
    log(f"Subscribed to topic: {MQTT_TOPIC}")

def on_message(client, userdata, msg):
    log("🔔 Message received")
    try:
        payload = json.loads(msg.payload.decode("utf-8"))
        log(f"🔍 Raw payload: {msg.payload[:80]}...")

        vp = payload.get("VP", {})
        if not vp:
            log("⚠️  No 'VP' key, skipping insert")
            return

        record = {
            "desi": vp.get("desi"),
            "dir": vp.get("dir"),
            "oper": vp.get("oper"),
            "veh": vp.get("veh"),
            "tst": vp.get("tst"),
            "tsi": vp.get("tsi"),
            "spd": vp.get("spd"),
            "hdg": vp.get("hdg"),
            "lat": vp.get("lat"),
            "long": vp.get("long"),
            "acc": vp.get("acc"),
            "dl": vp.get("dl"),
            "odo": vp.get("odo"),
            "drst": vp.get("drst"),
            "oday": vp.get("oday"),
            "jrn": vp.get("jrn"),
            "line": vp.get("line"),
            "start": vp.get("start"),
            "loc": vp.get("loc"),
            "stop": vp.get("stop"),
            "route": vp.get("route"),
            "occu": vp.get("occu"),
        }

        log(f"✅ Inserting record for veh={record['veh']} tst={record['tst']}")

        conn = psycopg2.connect(
            host=DB_HOST,
            port=DB_PORT,
            dbname=DB_NAME,
            user=DB_USER,
            password=DB_PASS,
        )
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO mqtt_hfp (desi, dir, oper, veh, tst, tsi, spd, hdg, lat, long, acc,
                                  dl, odo, drst, oday, jrn, line, start, loc, stop, route, occu)
            VALUES (%(desi)s, %(dir)s, %(oper)s, %(veh)s, %(tst)s, %(tsi)s, %(spd)s, %(hdg)s,
                    %(lat)s, %(long)s, %(acc)s, %(dl)s, %(odo)s, %(drst)s, %(oday)s, %(jrn)s,
                    %(line)s, %(start)s, %(loc)s, %(stop)s, %(route)s, %(occu)s)
        """, record)
        conn.commit()
        cur.close()
        conn.close()

        log("✔️ Insert successful")

    except Exception as e:
        log(f"❌ Error inserting message: {str(e)}")

# MQTT setup
client = mqtt.Client()
client.on_connect = on_connect
client.on_message = on_message

log("🚀 Starting MQTT client loop")
client.connect(MQTT_BROKER, MQTT_PORT, 60)
client.loop_forever()

=== FILE: ingestion/mqtt_hfp_ingest/requirements.txt ===
paho-mqtt
psycopg2-binary
SQLAlchemy
python-dotenv


=== FILE: ingestion/vehicle_positions_ingest.py ===
import requests
import time
import psycopg2
from google.transit import gtfs_realtime_pb2
import config

GTFS_VEHICLE_URL = config.GTFS_VEHICLE_URL
DB_HOST = config.DB_HOST
DB_PORT = config.DB_PORT
DB_NAME = config.DB_NAME
DB_USER = config.DB_USER
DB_PASS = config.DB_PASS

def fetch_and_store():
    print("Connecting to database...")
    conn = psycopg2.connect(
        host=DB_HOST, port=DB_PORT, dbname=DB_NAME,
        user=DB_USER, password=DB_PASS
    )
    cur = conn.cursor()
    print("Connected to database.")

    try:
        print("Fetching GTFS realtime feed...")
        response = requests.get(GTFS_VEHICLE_URL)
        response.raise_for_status()
        print("Feed fetched successfully, parsing...")
        feed = gtfs_realtime_pb2.FeedMessage()
        feed.ParseFromString(response.content)
        print(f"Parsed feed: {len(feed.entity)} entities")

        for entity in feed.entity:
            if not entity.HasField("vehicle"):
                continue

            vp = entity.vehicle
            vehicle_id = vp.vehicle.id
            route_id = vp.trip.route_id
            lat = vp.position.latitude
            lon = vp.position.longitude
            bearing = vp.position.bearing
            speed = vp.position.speed
            timestamp = vp.timestamp

            cur.execute("""
                INSERT INTO vehicle_positions (vehicle_id, route_id, lat, lon, bearing, speed, timestamp)
                VALUES (%s, %s, %s, %s, %s, %s, to_timestamp(%s))
            """, (vehicle_id, route_id, lat, lon, bearing, speed, timestamp))

        conn.commit()
        print("Commit done.")

    except Exception as e:
        print(f"Error during fetch or insert: {e}")
    finally:
        cur.close()
        conn.close()

if __name__ == "__main__":
    while True:
        fetch_and_store()
        time.sleep(5)

=== FILE: init_timescale.sql ===
CREATE EXTENSION IF NOT EXISTS timescaledb;

DROP TABLE IF EXISTS vehicle_positions;

CREATE TABLE vehicle_positions (
    id SERIAL,
    vehicle_id TEXT,
    route_id TEXT,
    lat DOUBLE PRECISION,
    lon DOUBLE PRECISION,
    bearing DOUBLE PRECISION,
    speed DOUBLE PRECISION,
    timestamp TIMESTAMPTZ NOT NULL
);
-- vehicles
CREATE TABLE IF NOT EXISTS vehicles (
    vehicle_id TEXT PRIMARY KEY,
    label TEXT,
    vehicle_type INTEGER,
    capacity INTEGER
);

-- alerts
CREATE TABLE IF NOT EXISTS alerts (
    alert_id SERIAL PRIMARY KEY,
    header_text TEXT,
    description_text TEXT,
    active_start TIMESTAMPTZ,
    active_end TIMESTAMPTZ
);

-- calendar
CREATE TABLE IF NOT EXISTS calendar (
    service_id TEXT PRIMARY KEY,
    monday BOOLEAN,
    tuesday BOOLEAN,
    wednesday BOOLEAN,
    thursday BOOLEAN,
    friday BOOLEAN,
    saturday BOOLEAN,
    sunday BOOLEAN,
    start_date DATE,
    end_date DATE
);

-- fare_attributes
CREATE TABLE IF NOT EXISTS fare_attributes (
    fare_id TEXT PRIMARY KEY,
    price NUMERIC,
    currency_type TEXT,
    payment_method INTEGER,
    transfers INTEGER
);

-- fare_rules
CREATE TABLE IF NOT EXISTS fare_rules (
    fare_id TEXT,
    origin_id TEXT,
    destination_id TEXT,
    contains_id TEXT
);

-- transfers
CREATE TABLE IF NOT EXISTS transfers (
    from_stop_id TEXT,
    to_stop_id TEXT,
    transfer_type INTEGER,
    min_transfer_time INTEGER
);

-- feed_info
CREATE TABLE IF NOT EXISTS feed_info (
    feed_publisher_name TEXT,
    feed_publisher_url TEXT,
    feed_lang TEXT,
    feed_start_date DATE,
    feed_end_date DATE,
    feed_version TEXT
);

-- emissions
CREATE TABLE IF NOT EXISTS emissions (
    vehicle_id TEXT,
    emission_type TEXT,
    emission_value NUMERIC
);
SELECT create_hypertable('vehicle_positions', 'timestamp', if_not_exists => TRUE, create_default_indexes => FALSE);


=== FILE: main.py ===
import os
import json
import ssl
import time
import paho.mqtt.client as mqtt
from sqlalchemy import create_engine, MetaData, Table, Column, String, Float, TIMESTAMP
from sqlalchemy.dialects.postgresql import insert

# DB Config (env based)
DB_HOST = os.getenv("DB_HOST", "db")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "hslbussit")
DB_USER = os.getenv("DB_USER", "postgres")
DB_PASS = os.getenv("DB_PASS", "supersecurepassword")

# DB Engine
engine = create_engine(f"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}")
metadata = MetaData()

# Create table definition
mqtt_hfp = Table(
    'mqtt_hfp', metadata,
    Column('tst', TIMESTAMP, primary_key=True),
    Column('veh', String),
    Column('desi', String),
    Column('dir', String),
    Column('lat', Float),
    Column('long', Float),
    Column('spd', Float),
    Column('hdg', Float),
    Column('dl', Float),
    Column('odo', Float),
    Column('route', String),
    Column('oper', String),
)

# Create table if not exists
metadata.create_all(engine)

# MQTT Callback
def on_message(client, userdata, msg):
    payload_str = msg.payload.decode('utf-8', errors='replace')
    print("🔍 Raw payload:", payload_str)
    try:
        payload = json.loads(payload_str)
    except Exception as e:
        print("❌ JSON parse error:", e)
        return

    print("🔑 Payload keys:", list(payload.keys()))
    if 'VP' not in payload:
        print("⚠️  Skipping insert, no 'VP' in payload")
        return

    v = payload['VP']
    print("✅ Inserting record:", v)
    try:
        with engine.begin() as conn:
            stmt = insert(mqtt_hfp).values(
                tst=v.get('tst'),
                veh=v.get('veh'),
                desi=v.get('desi'),
                dir=v.get('dir'),
                lat=v.get('lat'),
                long=v.get('long'),
                spd=v.get('spd'),
                hdg=v.get('hdg'),
                dl=v.get('dl'),
                odo=v.get('odo'),
                route=v.get('route'),
                oper=v.get('oper'),
            ).on_conflict_do_nothing()
            conn.execute(stmt)
            print("✔️  Insert succeeded")
    except Exception as e:
        print(f"❌ Error inserting message: {e}")

# MQTT Client Setup
client = mqtt.Client(mqtt.CallbackAPIVersion.VERSION2)
client.tls_set(cert_reqs=ssl.CERT_REQUIRED)
client.username_pw_set(username="", password="")  # no credentials needed
client.on_message = on_message

# Connect to broker
print("🚀 MQTT listener starting…")
client.connect("mqtt.hsl.fi", 8883)
client.subscribe("/hfp/v2/journey/ongoing/#")

# Start loop forever
client.loop_forever()


=== FILE: README.md ===
# Bussikartta

Real-time public transport map for the HSL region. This project shows live bus locations on a blazing fast vector tile map and uses WebSocket for high-frequency updates.

---

## ✨ Features

- SolidJS + Vite frontend (ultra fast, minimal re-renders)
- MapLibre GL JS map engine
- Supports 5000+ vehicle markers updated via WebSocket
- HSL vector tile support (online) or local `.mbtiles` via Tileserver-GL
- Fully containerized (Docker) backend and infrastructure

---

## 🧱 Repository Structure

```
repo/
├── api/                  # Python backend (FastAPI-ready)
├── gtfs/                 # GTFS static + realtime tools
├── scripts/              # CLI utilities
├── tileserver/           # Optional offline tile server (vector)
├── docker-compose.yml    # Service orchestration
├── README.md             # This file
└── docs/                 # Architecture, development and data notes
```

---

## 🚀 Quickstart (Dev Mode)

```bash
git clone https://github.com/YOUR_ORG/bussikartta.git
cd bussikartta
docker compose up -d
```

Then visit:

- Frontend (dev): http://localhost:5173
- Backend REST: http://localhost:8007/vehicles
- Backend WebSocket: ws://localhost:8007/ws
- (Optional) Offline tiles: http://localhost:8080

---

## 📡 Backend API

### `/vehicles` (GET)

Returns array of vehicle objects:

```json
[
  {
    "vehicle_id": "1234",
    "label": "600N",
    "lat": 60.17,
    "lon": 24.94,
    "speed": 33.0,
    "timestamp": "2025-06-19T00:00:00Z"
  }
]
```

### `/ws` (WebSocket)

Streams JSON payloads every second with updated vehicle positions.

---

## 🔌 Map Tiles

- **Online**: HSL Vector Tiles from `cdn.digitransit.fi`
- **Offline**: Generated via Tilemaker → served via Tileserver-GL
- Compatible with OpenMapTiles & MapLibre GL JS

---

## 📄 Documentation

- `docs/project_architecture_plan.md` — Locked architecture plan
- `docs/frontend_architecture.md` — Frontend layout + rendering logic
- `docs/services.md` — Service & port overview
- `docs/gtfs_data_handling.md` — Vehicle data formats

---

## 🐳 Docker Containers

| Name               | Role                    |
| ------------------|-------------------------|
| `repo-api-server` | Backend (`/vehicles`, `/ws`) |
| `bussikartta-ui`  | Frontend (static build) |
| `bussikartta-map` | Optional tile server    |

---

## 👨‍💻 Dev Scripts

```bash
docker compose logs -f
docker compose exec api bash
npm run dev        # inside frontend if dev server is needed
```

---

© HSL Bussikartta 2025


=== FILE: tools/mqtt_watchdog.sh ===
#!/bin/sh

CONTAINER_NAME="mqtt-ingest"
DB_CONTAINER="repo-db-1"
MAX_LAG_SECONDS=120

check_container_running() {
  docker inspect -f '{{.State.Running}}' "$CONTAINER_NAME" 2>/dev/null | grep true >/dev/null
}

check_db_lag_ok() {
  LAG=$(docker exec "$DB_CONTAINER" psql -U postgres -d hslbussit -t -c "SELECT EXTRACT(EPOCH FROM NOW() - MAX(tst)) FROM mqtt_hfp;" | tr -d '[:space:]')
  [ "$LAG" != "" ] && [ "$(printf '%.0f' "$LAG")" -lt "$MAX_LAG_SECONDS" ]
}

restart_container() {
  echo "[watchdog] Restarting $CONTAINER_NAME due to failure"
  docker restart "$CONTAINER_NAME"
}

log_status() {
  echo "$(date '+%Y-%m-%d %H:%M:%S') [watchdog] $1"
}

if ! check_container_running; then
  log_status "$CONTAINER_NAME is not running"
  restart_container
  exit 1
fi

if ! check_db_lag_ok; then
  log_status "DB insert lag too high"
  restart_container
  exit 2
fi

log_status "All OK"
exit 0


=== FILE: tools/vehicle_watchdog.sh ===
#!/bin/sh

log() {
  echo "$(date +'%F %T') [vehicle-watchdog] $1"
}

# Run SQL to determine lag in seconds
lag_sec=$(docker exec repo-db-1 psql -U postgres -d hslbussit -t -A -c \
"SELECT EXTRACT(EPOCH FROM (NOW() - MAX(timestamp))) FROM vehicle_positions;" 2>/dev/null)

# If result is empty or too large, trigger restart
if [ -z "$lag_sec" ]; then
  log "No data available, restarting vehicle-ingest"
  docker restart vehicle-ingest
elif [ "$(echo "$lag_sec > 90" | bc)" -eq 1 ]; then
  log "Insert lag too high (${lag_sec}s), restarting vehicle-ingest"
  docker restart vehicle-ingest
else
  log "OK – lag ${lag_sec}s"
fi



# Ignored Files:
!! .DS_Store
!! .env
!! @eaDir/
!! api/.DS_Store
!! api/@eaDir/
!! api/__pycache__/
!! api/routes/@eaDir/
!! api/routes/__pycache__/
!! backups/
!! dbdata/
!! docs/@eaDir/
!! gtfs_static/@eaDir/
!! ingestion/.DS_Store
!! ingestion/@eaDir/
!! ingestion/__pycache__/
!! ingestion/mqtt_hfp_ingest/@eaDir/
